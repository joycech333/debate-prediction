import util
import os
import numpy as np
import gensim.downloader as api
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
# Remember to run pip install xgboost
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

import re
import json

files = [f.name for f in os.scandir("scraped-data/transcripts")]
# Load Google's pre-trained Word2Vec model
model = api.load("word2vec-google-news-300") 

'''
Get labels from winners file. Since this is associated with each file (debate), 
we need to build the labels vector as we read each file from files.
Note that this code can be integrated with the function make_speakers_dict,
but is separate for readability.
'''
def get_winners(files):
    labels = []
    with open('data/ground_truths.json') as json_file:
        winners = json.load(json_file)

    for file_path in files:
        speakers_lines = util.split_speakers(file_path)
        for speaker in speakers_lines:
            # Janky way to fix the discrepancy between file names in the path and ground_truths.json
            winner = winners[file_path[-14:]]
            # This one splits on the second forward slash and returns last part
            # winner = winners[file_path].rsplit('/', 1)[-1]
            # If this speaker is the winner of the debate, represent as a 1 in labels vector
            if speaker.lower() == winner.lower():
                labels.append([1])
            else:
                labels.append([0])
    return np.array(labels)

'''
Takes in a single line of text, returns tokenized vector of cleaned words from line.
Remove punctuations; numbers; English stopwords; tokenize and lowercase.
'''
def preprocess_text(line):
    cleaned_line = re.sub(r'[^\w\s]', '', line)
    cleaned_line = re.sub(r'\d+', '', cleaned_line) 
    tokens = word_tokenize(cleaned_line.lower())
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    return filtered_tokens

'''
Takes in all debate files, returns a dictionary containing words spoken by each speaker. 
Also returns the labels vector.
Keys = names of speakers.
Values = list of all tokenized & processed words from that person's speech, including duplicates
(tokenized concatenation of all words the speaker spoke). e.g. ['word1', 'word2', 'word3']
(Casual explanation: each speaker has one huge list of all the words they spoke)
'''
def make_speakers_dict(files):
    speakers_words = {}

    #Â Using integers as keys to allow duplicate speakers
    i = 0
    for file_path in files:
        speakers_lines = util.split_speakers(file_path)
        
        for speaker, lines in speakers_lines.items():
            speakers_words[i] = [(preprocess_text(line)) for line in lines]
            i += 1

    return speakers_words

'''
Test make_speakers_dict
'''
# dict = make_speakers_dict(files)
# print(dict.keys())
# print(list(dict.values())[0])

'''
Input: dictionary of tokenized speakers' words.
Using the pre-trained Word2Vec model, get embeddings of every word for every speaker.
Takes mean of the word embeddings to calculate feature vector for that speaker.
Output: numpy array of feature vectors, one for each speaker. These are the Xs.
'''
def make_features(speakers_words):
    vocab = list(model.index_to_key)
    
    features = []
    for speaker, tokens in speakers_words.items():
        speaker_vec = []
        for word in tokens:
            if word in vocab: 
                speaker_vec.append(model[word])
            else:
                # This speaker word is not in the pre-train vocab
                speaker_vec.append(np.zeros(300))
        speaker_feature = np.mean(speaker_vec, axis=0)
        features.append(speaker_feature)
    return np.array(features)

'''
Test whether embed worked
'''
# print(make_features(make_speakers_dict(files)).shape)
# make_features(make_speakers_dict(files))

'''
Run models
'''
features = make_features(make_speakers_dict(files))
labels = get_winners(files)
model = LogisticRegression()
# model = XGBClassifier(n_estimators=100, random_state=42)
# model = RandomForestClassifier(n_estimators=100, random_state=42)

def cross_val_accuracy(features, labels, model):

    # Perform 10-fold cross-validation
    cv_scores = cross_val_score(model, features, labels, cv=10)

    # Print accuracy for each fold
    print("Accuracy for each fold:", cv_scores)

    # Calculate mean accuracy across folds
    mean_accuracy = np.mean(cv_scores)
    print(f"Mean Accuracy: {mean_accuracy}")

cross_val_accuracy(features, labels, model)
